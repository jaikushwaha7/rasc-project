{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289fac00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Demo: Step-by-step RASC inference\n",
    "Shows objects, relationships, and caption generation in real-time\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from src.models.inference import RASCInference  # your RASCInference class\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "CONFIG_PATH = \"configs/config.yaml\"  # Path to your config file\n",
    "IMAGE_PATH = \"sample_images/living_room.jpg\"  # Example image\n",
    "YOLO_WEIGHTS = \"runs/detect/experiments/runs/yolo_experiment_1/weights/best.pt # Defaults to yolov8n.pt\"\n",
    "RELATIONSHIP_WEIGHTS = \"models/relationship_predictor/neural_motifs_best.pt\"  # Defaults to neural_motifs.pt\n",
    "CAPTION_MODEL = \"models/caption_generator/t5_scene_best.pt\"  # Defaults to t5_scene\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize pipeline\n",
    "# -----------------------------\n",
    "pipeline = RASCInference(\n",
    "    config_path=CONFIG_PATH,\n",
    "    yolo_weights=YOLO_WEIGHTS,\n",
    "    relationship_weights=RELATIONSHIP_WEIGHTS,\n",
    "    caption_model=CAPTION_MODEL\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Detect Objects\n",
    "# -----------------------------\n",
    "print(\"\\n=== Step 1: Object Detection ===\")\n",
    "objects = pipeline.detect_objects(IMAGE_PATH)\n",
    "for i, (cls_id, bbox) in enumerate(objects):\n",
    "    print(f\"Object {i}: Class={cls_id}, BBox={bbox.tolist()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Predict Relationships\n",
    "# -----------------------------\n",
    "print(\"\\n=== Step 2: Relationship Prediction ===\")\n",
    "relationships = pipeline.predict_relationships(objects)\n",
    "for rel in relationships:\n",
    "    print(rel)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Generate Caption\n",
    "# -----------------------------\n",
    "print(\"\\n=== Step 3: Caption Generation ===\")\n",
    "caption = pipeline.generate_caption(relationships)\n",
    "print(f\"Generated Caption: {caption}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Full Pipeline Results\n",
    "# -----------------------------\n",
    "print(\"\\n=== Step 4: Full Pipeline Output ===\")\n",
    "results = pipeline.run(IMAGE_PATH, verbose=True)\n",
    "\n",
    "# Optionally save results\n",
    "OUTPUT_PATH = Path(\"results/demo_output.json\")\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ceca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
